{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeekshaKarkada/Language-Translation-for-Hindi-Kannada/blob/main/LSTM_OpNMT_BM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing Language translation from Hindi to Kannada using LSTM\n",
        "\n",
        "using https://github.com/ymoslem/MT-Preparation/tree/main as reference"
      ],
      "metadata": {
        "id": "xCbtBug1tnIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P4WUdDb_Vgq",
        "outputId": "fbc5b4c8-3932-4122-ebbb-0065cf83c6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install OpenNMT-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq1nqbS42v8J",
        "outputId": "3df275a8-e384-4836-9b11-128fc8596110"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting torch<2.3,>=2.1 (from OpenNMT-py)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.17.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.2)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Collecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.13.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (7.0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (0.1.2)\n",
            "Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.4-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: waitress, triton, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, nvidia-cusolver-cu12, torch, OpenNMT-py\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.3.1 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.1 pyahocorasick-2.1.0 pybind11-2.13.4 pyonmttok-1.37.1 rapidfuzz-3.9.6 sacrebleu-2.4.3 torch-2.2.2 triton-2.2.0 waitress-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AC-7skGl_aNs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC2AmpQl_I-X",
        "outputId": "82161b42-7783-4b8c-8df5-53df2fea768b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKTYeIvR-2ju",
        "outputId": "deba6e68-75cb-4b33-a927-e60b1482d170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt\n"
          ]
        }
      ],
      "source": [
        "# Create a directory and clone the Github MT-Preparation repository\n",
        "!mkdir nmt\n",
        "%cd nmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eFPoYYEj_Ng4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data_path = '/content/drive/MyDrive/Thesis/Opennmt_Files/'\n",
        "os.chdir(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ox3Dq6aD4TLe",
        "outputId": "8dc5d248-efaa-4efb-afdf-8fdeaf57446a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Thesis/Opennmt_Files'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skWETYPb_O-Q",
        "outputId": "88d46969-40d2-4910-925c-34993176bb9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-18 14:39:37--  https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/hi-kn.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116758012 (111M) [application/zip]\n",
            "Saving to: ‘hi-kn.txt.zip’\n",
            "\n",
            "hi-kn.txt.zip       100%[===================>] 111.35M  16.9MB/s    in 7.8s    \n",
            "\n",
            "2024-08-18 14:39:46 (14.3 MB/s) - ‘hi-kn.txt.zip’ saved [116758012/116758012]\n",
            "\n",
            "Archive:  hi-kn.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: NLLB.hi-kn.hi           \n",
            "  inflating: NLLB.hi-kn.kn           \n",
            "  inflating: NLLB.hi-kn.scores       \n"
          ]
        }
      ],
      "source": [
        "# Download and unzip a dataset\n",
        "!wget https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/hi-kn.txt.zip\n",
        "!unzip hi-kn.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def data_preprocessing(source_file, target_file, source_lang, target_lang, lower=False):\n",
        "\n",
        "    data_source = pd.read_csv(source_file, names=['Source'], sep=\"\\0\", skip_blank_lines=False, on_bad_lines=\"skip\")\n",
        "    data_target = pd.read_csv(target_file, names=['Target'], sep=\"\\0\", skip_blank_lines=False, on_bad_lines=\"skip\")\n",
        "    data = pd.concat([data_source, data_target], axis=1)  # Join the two dataframes along columns\n",
        "\n",
        "    # Delete nan\n",
        "    data = data.dropna()\n",
        "\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    data = data.replace(r'<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|{}', ' ', regex=True)\n",
        "    data = data.replace(r'  ', ' ', regex=True)\n",
        "\n",
        "    data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Write the dataframe to two Source and Target files\n",
        "    source_file = source_file+'-filtered.'+source_lang\n",
        "    target_file = target_file+'-filtered.'+target_lang\n",
        "\n",
        "    data_source = data[\"Source\"]\n",
        "    data_target = data[\"Target\"]\n",
        "\n",
        "    data_source.to_csv(source_file, header=False, index=False, sep=\"\\n\")\n",
        "    print(\"Source file Saved:\", source_file)\n",
        "    data_target.to_csv(target_file, header=False, index=False, sep=\"\\n\")\n",
        "    print(\"Target file Saved:\", target_file)\n",
        "\n",
        "\n",
        "data_preprocessing('NLLB.hi-kn.hi', 'NLLB.hi-kn.kn', 'hi', 'kn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKVpT8Or1taB",
        "outputId": "ff3ff933-6106-4811-db62-d67cef5ffea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Source Saved: NLLB.hi-kn.hi-filtered.hi\n",
            "--- Target Saved: NLLB.hi-kn.kn-filtered.kn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpNVjOKehcOw"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Source subword model\n",
        "\n",
        "source_train_value = '--input=/content/nmt/NLLB.hi-kn.hi --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(source_train_value)\n",
        "\n",
        "# Target subword model\n",
        "\n",
        "target_train_value = '--input=/content/nmt/NLLB.hi-kn.kn --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true'\n",
        "spm.SentencePieceTrainer.train(target_train_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGh7-EKrgTIG",
        "outputId": "ba622d79-b9a2-46ce-c538-64b277593d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: NLLB.hi-kn.hi-filtered.hi\n",
            "Target Dataset: NLLB.hi-kn.kn-filtered.kn\n",
            "Done subwording the source file! Output: NLLB.hi-kn.hi-filtered.hi.subword\n",
            "Done subwording the target file! Output: NLLB.hi-kn.kn-filtered.kn.subword\n"
          ]
        }
      ],
      "source": [
        "# Train a SentencePiece model for subword tokenization using subword file from https://github.com/ymoslem/MT-Preparation/tree/main\n",
        "!python3 2-subword.py source.model target.model NLLB.hi-kn.hi-filtered.hi NLLB.hi-kn.kn-filtered.kn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwP7OMAnBJa",
        "outputId": "3f43e191-af05-4ec3-a212-81841d00941a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ग्लोबल ▁जापानी ▁कार्यक्रम ▁के ▁इस ▁सप्ताह ▁के ▁भाग ▁में , ▁हम ▁इस ▁बारे ▁में ▁बात ▁करते ▁हैं ▁कि ▁जापान ▁दुनिया ▁भर ▁की ▁तकनीक ▁में ▁अपना ▁अनुभव ▁कैसे ▁साझा ▁करता ▁है ।\n",
            "▁मैंने ▁उसी ▁के ▁साथ ▁एक ▁होटल ▁में ▁खाना ▁खाया ▁और ▁दोस्त ▁को ▁बाय ▁बोलकर ▁निकल ▁गया .\n",
            "▁परिपक्व ▁माँ ▁अपने ▁बेटे ▁के ▁दोस्त ▁Fucks\n",
            "▁एक ▁कदम ▁से ▁नफरत ▁करने ▁के ▁लिए ▁प्यार ▁से !\n",
            "▁इसमें ▁उनकी ▁मां ▁ने ▁हर ▁कदम ▁पर ▁उनका ▁साथ ▁दिया ।\n"
          ]
        }
      ],
      "source": [
        "# First 3 lines before subwording\n",
        "!head -n 5 NLLB.hi-kn.hi-filtered.hi.subword"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 NLLB.hi-kn.kn-filtered.kn.subword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj7ERxG3xGEL",
        "outputId": "5b832c77-c5ff-400f-88ed-ccffd935c9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ಜಪಾನ್ನ ▁ವೇಗವಾಗಿ ▁ಯುರೋಪಿಯನ್ ▁ಮಾರುಕಟ್ಟೆಯಲ್ಲಿ ▁ಶಿ ಂ ಕಾನ್ ಸೆನ್ಸ್ ಗೆ ▁ತರಬೇತಿ ▁ನೀಡುತ್ತದೆ ▁ 2 8 ▁/ ▁ 0 7 ▁/ ▁ 2 0 1 7 ▁ಜಾಗತಿಕ ▁ಜಪಾನೀಸ್ ▁ಕಾರ್ಯಕ್ರಮದ ▁ಈ ▁ವಾರದ ▁ಭಾಗದಲ್ಲಿ , ▁ಜಪಾನ್ ▁ತನ್ನ ▁ತಂತ್ರಜ್ಞಾನದ ▁ಅನುಭವವನ್ನು ▁ವಿಶ್ವದಾದ್ಯಂತ ▁ಹೇಗೆ ▁ಹಂಚಿಕೊಳ್ಳ ುತ್ತದೆ ▁ಎಂಬುದರ ▁ಕುರಿತು ▁ನಾವು ▁ಮಾತನಾಡುತ್ತೇವೆ .\n",
            "▁ನಾನು ▁ಸ್ನೇಹಿತ ನೊಂದಿಗೆ ▁ಹೋ ಟೆಲಿ ಗೆ ▁ಚಹಾ ▁ಕುಡಿಯಲು ▁ಹೋಗಿದ್ದೆ ▁ಅಲ್ಲಿಗೆ ▁ಅವನೂ ▁ಬಂದಿದ್ದ ▁ಅವನ ▁ಸ್ನೇಹಿತ ನೊಂದಿಗೆ .\n",
            "▁\"\"\" ▁ಪ್ರೌಢ ▁ಸೂಳೆ ▁ತಾಯಿ ▁ತನ್ನ ▁ಮಗನ ▁ಸ್ನೇಹಿತ ▁Fucks ▁\"\"\"\n",
            "▁ಆಶ್ಚರ್ಯ ▁ಮತ್ತು ▁ಪ್ರತಿಯಾಗಿ ▁\" ಒಂದು ▁ಹೆಜ್ಜೆ ▁ದ್ವೇಷಿಸಲು ▁ಪ್ರೀತಿ ▁ಗೆ \" ▁ಒಂದು ▁ಗಾದೆ ▁ಇದೆ .\n",
            "▁ಆದರೆ ▁ಅವರ ▁ತಾಯಿ ▁ಮಗಳ ▁ಪ್ರತಿ ▁ಹೆಜ್ಜೆಗೂ ▁ಜೊತೆಯಾಗಿ ▁ನಿಂತರು .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF4Tq-dgnJzo",
        "outputId": "445ecb39-e2f2-431f-805c-ff2919f2923f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (1793892, 2)\n",
            "Output files\n",
            "NLLB.hi-kn.hi-filtered.hi.subword.train\n",
            "NLLB.hi-kn.kn-filtered.kn.subword.train\n",
            "NLLB.hi-kn.hi-filtered.hi.subword.dev\n",
            "NLLB.hi-kn.kn-filtered.kn.subword.dev\n",
            "NLLB.hi-kn.hi-filtered.hi.subword.test\n",
            "NLLB.hi-kn.kn-filtered.kn.subword.test\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "\n",
        "def split_dataset(segment_no_dev, segment_no_test, source_file, target_file):\n",
        "\n",
        "    df_source = pd.read_csv(source_file,\n",
        "                            names=['Source'],\n",
        "                            sep=\"\\0\",\n",
        "                            quoting=csv.QUOTE_NONE,\n",
        "                            skip_blank_lines=False,\n",
        "                            on_bad_lines=\"skip\")\n",
        "    df_target = pd.read_csv(target_file,\n",
        "                            names=['Target'],\n",
        "                            sep=\"\\0\",\n",
        "                            quoting=csv.QUOTE_NONE,\n",
        "                            skip_blank_lines=False,\n",
        "                            on_bad_lines=\"skip\")\n",
        "    data = pd.concat([df_source, df_target], axis=1)\n",
        "    print(\"Dataframe shape:\", data.shape)\n",
        "\n",
        "    data = data.dropna()\n",
        "\n",
        "\n",
        "    # Extract Dev set from the main dataset\n",
        "    data_dev = data.sample(n = int(segment_no_dev))\n",
        "    data_train = data.drop(data_dev.index)\n",
        "\n",
        "    # Extract Test set from the main dataset\n",
        "    data_test = data_train.sample(n = int(segment_no_test))\n",
        "    data_train = data_train.drop(data_test.index)\n",
        "\n",
        "    # Write the dataframe to two Source and Target files\n",
        "    source_file_train = source_file+'.train'\n",
        "    target_file_train = target_file+'.train'\n",
        "\n",
        "    source_file_dev = source_file+'.dev'\n",
        "    target_file_dev = target_file+'.dev'\n",
        "\n",
        "    source_file_test = source_file+'.test'\n",
        "    target_file_test = target_file+'.test'\n",
        "\n",
        "    data_dic_train = data_train.to_dict(orient='list')\n",
        "\n",
        "\n",
        "    with open(source_file_train, \"w\") as sf:\n",
        "        sf.write(\"\\n\".join(line for line in data_dic_train['Source']))\n",
        "        sf.write(\"\\n\")\n",
        "\n",
        "    with open(target_file_train, \"w\") as tf:\n",
        "        tf.write(\"\\n\".join(line for line in data_dic_train['Target']))\n",
        "        tf.write(\"\\n\")\n",
        "\n",
        "\n",
        "    data_dic_dev = data_dev.to_dict(orient='list')\n",
        "\n",
        "    with open(source_file_dev, \"w\", encoding='utf-8') as sf:\n",
        "        sf.write(\"\\n\".join(line for line in data_dic_dev['Source']))\n",
        "        sf.write(\"\\n\") # end of file newline\n",
        "\n",
        "    with open(target_file_dev, \"w\", encoding='utf-8') as tf:\n",
        "        tf.write(\"\\n\".join(line for line in data_dic_dev['Target']))\n",
        "        tf.write(\"\\n\")\n",
        "\n",
        "\n",
        "    data_dic_test = data_test.to_dict(orient='list')\n",
        "\n",
        "    with open(source_file_test, \"w\", encoding='utf-8') as sf:\n",
        "        sf.write(\"\\n\".join(line for line in data_dic_test['Source']))\n",
        "        sf.write(\"\\n\")\n",
        "\n",
        "    with open(target_file_test, \"w\", encoding='utf-8') as tf:\n",
        "        tf.write(\"\\n\".join(line for line in data_dic_test['Target']))\n",
        "        tf.write(\"\\n\")\n",
        "\n",
        "    print(\"Output files\", *[source_file_train, target_file_train, source_file_dev, target_file_dev, source_file_test, target_file_test], sep=\"\\n\")\n",
        "\n",
        "\n",
        "segment_no_dev = 6000    # Number of segments in the dev set\n",
        "segment_no_test = 6000    # Number of segments in the test set\n",
        "source_file = 'NLLB.hi-kn.hi-filtered.hi.subword'   # Path to the source file\n",
        "target_file = 'NLLB.hi-kn.kn-filtered.kn.subword'   # Path to the target file\n",
        "\n",
        "split_dataset(segment_no_dev, segment_no_test, source_file, target_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW0LWjqAsde4"
      },
      "outputs": [],
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: NLLB.hi-kn.hi-filtered.hi.subword.train\n",
        "        path_tgt: NLLB.hi-kn.kn-filtered.kn.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: NLLB.hi-kn.hi-filtered.hi.subword.dev\n",
        "        path_tgt: NLLB.hi-kn.kn-filtered.kn.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "log_file: train.log\n",
        "save_model: models/model.hikn\n",
        "\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 -\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "\n",
        "train_steps: 10000\n",
        "valid_steps: 5000\n",
        "\n",
        "\n",
        "warmup_steps: 1000\n",
        "report_every: 1000\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "#world_size: 1\n",
        "#gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: rnn\n",
        "decoder_type: rnn\n",
        "rnn_type: LSTM\n",
        "position_encoding: true\n",
        "enc_layers: 2\n",
        "dec_layers: 2\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.3]\n",
        "attention_dropout: [0.1]\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the vocabulary using the config file and for the vocalbulary size as in config file"
      ],
      "metadata": {
        "id": "mZsSmZEMwQzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afbef62-bf7d-43f5-a539-cf274b94a126",
        "id": "LZ_ci16Fvt4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-08-21 13:03:18,432 INFO] Counter vocab from -1 samples.\n",
            "[2024-08-21 13:03:18,432 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-08-21 13:04:04,810 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 13:04:04,917 INFO] Counters src: 52785\n",
            "[2024-08-21 13:04:04,917 INFO] Counters tgt: 52079\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model for the source and taget files and other hyper parameters as set in config file"
      ],
      "metadata": {
        "id": "_JFxavktwaF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe88db5f-15a2-4590-a84a-9ac5d767a3df",
        "id": "7BOU3TD0v2_C"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-08-21 13:04:35,623 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-08-21 13:04:35,623 INFO] Parsed 2 corpora from -data.\n",
            "[2024-08-21 13:04:35,624 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-08-21 13:04:35,856 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '।', '▁', '▁के', '▁है', '▁में', ',']\n",
            "[2024-08-21 13:04:35,857 INFO] The decoder start token is: <s>\n",
            "[2024-08-21 13:04:35,857 INFO] Building model...\n",
            "[2024-08-21 13:04:37,196 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-08-21 13:04:37,197 INFO] Non quantized layer compute is fp16\n",
            "[2024-08-21 13:04:37,429 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (rnn): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1024, 512)\n",
            "        (1): LSTMCell(512, 512)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_in): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=50000, bias=True)\n",
            ")\n",
            "[2024-08-21 13:04:37,430 INFO] encoder: 29802496\n",
            "[2024-08-21 13:04:37,430 INFO] decoder: 57287504\n",
            "[2024-08-21 13:04:37,430 INFO] * number of parameters: 87090000\n",
            "[2024-08-21 13:04:37,430 INFO] Trainable parameters = {'torch.float32': 87090000, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-08-21 13:04:37,430 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-08-21 13:04:37,430 INFO]  * src vocab size = 50000\n",
            "[2024-08-21 13:04:37,430 INFO]  * tgt vocab size = 50000\n",
            "[2024-08-21 13:04:37,739 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-08-21 13:04:37,739 INFO] Starting training on GPU: [0]\n",
            "[2024-08-21 13:04:37,739 INFO] Start training loop and validate every 5000 steps...\n",
            "[2024-08-21 13:04:37,739 INFO] Scoring with: ['filtertoolong']\n",
            "[2024-08-21 13:13:32,297 INFO] Step 1000/10000; acc: 15.0; ppl: 1990.3; xent: 7.6; lr: 0.00279; sents: 1026477; bsz: 2709/3017/257; 20269/22579 tok/s;    535 sec;\n",
            "[2024-08-21 13:13:32,317 INFO] Saving checkpoint models/model.hikn_step_1000.pt\n",
            "[2024-08-21 13:18:24,317 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 13:18:24,318 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-08-21 13:22:36,847 INFO] Step 2000/10000; acc: 21.6; ppl: 721.2; xent: 6.6; lr: 0.00198; sents: 1026988; bsz: 2716/3020/257; 19953/22182 tok/s;   1079 sec;\n",
            "[2024-08-21 13:22:36,866 INFO] Saving checkpoint models/model.hikn_step_2000.pt\n",
            "[2024-08-21 13:31:40,547 INFO] Step 3000/10000; acc: 23.0; ppl: 579.8; xent: 6.4; lr: 0.00161; sents: 1030343; bsz: 2717/3024/258; 19991/22248 tok/s;   1623 sec;\n",
            "[2024-08-21 13:31:40,566 INFO] Saving checkpoint models/model.hikn_step_3000.pt\n",
            "[2024-08-21 13:34:32,654 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 13:34:32,655 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-08-21 13:40:48,618 INFO] Step 4000/10000; acc: 23.6; ppl: 522.0; xent: 6.3; lr: 0.00140; sents: 1029591; bsz: 2715/3018/257; 19814/22025 tok/s;   2171 sec;\n",
            "[2024-08-21 13:40:48,638 INFO] Saving checkpoint models/model.hikn_step_4000.pt\n",
            "[2024-08-21 13:49:54,837 INFO] Step 5000/10000; acc: 24.2; ppl: 480.9; xent: 6.2; lr: 0.00125; sents: 1023682; bsz: 2721/3011/256; 19923/22052 tok/s;   2717 sec;\n",
            "[2024-08-21 13:49:58,153 INFO] valid stats calculation\n",
            "                           took: 3.315312385559082 s.\n",
            "[2024-08-21 13:49:58,153 INFO] Train perplexity: 731.182\n",
            "[2024-08-21 13:49:58,154 INFO] Train accuracy: 21.4719\n",
            "[2024-08-21 13:49:58,154 INFO] Sentences processed: 5.13708e+06\n",
            "[2024-08-21 13:49:58,154 INFO] Average bsz: 2716/3018/257\n",
            "[2024-08-21 13:49:58,154 INFO] Validation perplexity: 433.735\n",
            "[2024-08-21 13:49:58,154 INFO] Validation accuracy: 25.0243\n",
            "[2024-08-21 13:49:58,154 INFO] Model is improving ppl: inf --> 433.735.\n",
            "[2024-08-21 13:49:58,154 INFO] Model is improving acc: -inf --> 25.0243.\n",
            "[2024-08-21 13:49:58,172 INFO] Saving checkpoint models/model.hikn_step_5000.pt\n",
            "[2024-08-21 13:50:51,871 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 13:50:51,871 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-08-21 13:59:08,585 INFO] Step 6000/10000; acc: 24.5; ppl: 459.4; xent: 6.1; lr: 0.00114; sents: 1024397; bsz: 2701/3015/256; 19514/21777 tok/s;   3271 sec;\n",
            "[2024-08-21 13:59:08,605 INFO] Saving checkpoint models/model.hikn_step_6000.pt\n",
            "[2024-08-21 14:07:03,800 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 14:07:03,801 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-08-21 14:08:15,800 INFO] Step 7000/10000; acc: 24.8; ppl: 438.6; xent: 6.1; lr: 0.00106; sents: 1027948; bsz: 2716/3025/257; 19856/22110 tok/s;   3818 sec;\n",
            "[2024-08-21 14:08:15,819 INFO] Saving checkpoint models/model.hikn_step_7000.pt\n",
            "[2024-08-21 14:17:25,493 INFO] Step 8000/10000; acc: 25.0; ppl: 426.3; xent: 6.1; lr: 0.00099; sents: 1028254; bsz: 2709/3019/257; 19712/21969 tok/s;   4368 sec;\n",
            "[2024-08-21 14:17:25,513 INFO] Saving checkpoint models/model.hikn_step_8000.pt\n",
            "[2024-08-21 14:21:10,816 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-08-21 14:21:10,817 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-08-21 14:26:34,559 INFO] Step 9000/10000; acc: 25.3; ppl: 414.6; xent: 6.0; lr: 0.00093; sents: 1029265; bsz: 2720/3015/257; 19815/21968 tok/s;   4917 sec;\n",
            "[2024-08-21 14:26:34,579 INFO] Saving checkpoint models/model.hikn_step_9000.pt\n",
            "[2024-08-21 14:35:43,749 INFO] Step 10000/10000; acc: 25.5; ppl: 404.3; xent: 6.0; lr: 0.00088; sents: 1027990; bsz: 2718/3019/257; 19796/21991 tok/s;   5466 sec;\n",
            "[2024-08-21 14:35:47,095 INFO] valid stats calculation\n",
            "                           took: 3.3456645011901855 s.\n",
            "[2024-08-21 14:35:47,095 INFO] Train perplexity: 559.544\n",
            "[2024-08-21 14:35:47,095 INFO] Train accuracy: 23.2422\n",
            "[2024-08-21 14:35:47,095 INFO] Sentences processed: 1.02749e+07\n",
            "[2024-08-21 14:35:47,095 INFO] Average bsz: 2714/3018/257\n",
            "[2024-08-21 14:35:47,095 INFO] Validation perplexity: 380.956\n",
            "[2024-08-21 14:35:47,096 INFO] Validation accuracy: 26.2734\n",
            "[2024-08-21 14:35:47,096 INFO] Model is improving ppl: 433.735 --> 380.956.\n",
            "[2024-08-21 14:35:47,096 INFO] Model is improving acc: 25.0243 --> 26.2734.\n",
            "[2024-08-21 14:35:47,114 INFO] Saving checkpoint models/model.hikn_step_10000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN55qovBvB8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75df58a6-23f3-488f-e7ba-ae1587c99def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-08-18 16:47:49,733 INFO] Loading checkpoint from models/model.fren_step_10000.pt\n",
            "[2024-08-18 16:48:00,062 INFO] Loading data into the model\n",
            "[2024-08-18 16:48:43,958 INFO] PRED SCORE: -0.8908, PRED PPL: 2.44 NB SENTENCES: 2000\n",
            "Time w/o python interpreter load/terminate:  54.826354026794434\n"
          ]
        }
      ],
      "source": [
        "# translate the sentences of Hindi test data for the trained model and store to another file\n",
        "\n",
        "!onmt_translate -model models/model.hikn_step_10000.pt -src NLLB.hi-kn.hi-filtered.hi.subword.test -output NLLB.kn.translated -min_length 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKnvhIRLvjn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91113223-e655-4cb2-a9d0-bd2eebfba7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ಉತ್ಪಾದಕತೆ ಗೆ ▁ಸಂಬಂಧಿಸಿದ ▁ಅಂಶಗಳು .\n",
            "▁ಕಂದಹಾರ್ ▁ವಿಮಾನ ▁ನಿಲ್ದಾಣದಲ್ಲಿ ▁ರಾಕೆಟ್ ▁ದಾಳಿ\n",
            "▁ಅಧ್ಯಯನ ▁ಮುಂದುವರೆದಿದೆ .\n"
          ]
        }
      ],
      "source": [
        "!head -n 3 NLLB.kn.translated"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detokenize the translated file and test file\n",
        "\n",
        "Reverse the tokenization process on the file containing tokenized translation results using a detokenizer function by loading the 'target_model' along with the tokenized file"
      ],
      "metadata": {
        "id": "UG9_5Genxnfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "def detokenization(target_model, pred_target):\n",
        "  target_decodeded = pred_target + \".desubword\"\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load(target_model)\n",
        "  with open(pred_target) as pred, open(target_decodeded, \"w+\") as pred_decoded:\n",
        "    for line in pred:\n",
        "        line = line.strip().split(\" \")\n",
        "        line = sp.decode_pieces(line)\n",
        "        pred_decoded.write(line + \"\\n\")\n",
        "\n",
        "  print(\"Desubword file:\", target_decodeded)\n",
        "\n"
      ],
      "metadata": {
        "id": "6xfhsEEH0rIt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenization('target.model', 'NLLB.kn.translated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YM05_Yx0n8d",
        "outputId": "3dce98da-082e-41f2-88b2-b66745e6c728"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desubword file: NLLB.kn.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 NLLB.kn.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFFpTcaRKkKh",
        "outputId": "fb03c775-82ec-49df-bbd0-bdfa8f83f2a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ಉತ್ಪಾದಕತೆಗೆ ಸಂಬಂಧಿಸಿದ ಅಂಶಗಳು.\n",
            "ಕಂದಹಾರ್ ವಿಮಾನ ನಿಲ್ದಾಣದಲ್ಲಿ ರಾಕೆಟ್ ದಾಳಿ\n",
            "ಅಧ್ಯಯನ ಮುಂದುವರೆದಿದೆ.\n",
            "\"ಈ ಮೊದಲು, ನನ್ನ ಹೆಂಡತಿಗೂ ಕೊರೋನಾ ಇರುವುದು ದೃಢಪಟ್ಟಿತ್ತು, ಆದರೆ ನಮ್ಮ\n",
            "ವೆಚ್ಚ, ಇತ್ಯಾದಿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detokenization('target.model', 'NLLB.hi-kn.kn-filtered.kn.subword.test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grxbVOM3K3dd",
        "outputId": "a2a8a8f8-f7ef-4764-f763-0c0282ae2c7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desubword file: NLLB.hi-kn.kn-filtered.kn.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 NLLB.hi-kn.kn-filtered.kn.subword.test.desubword\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPqFzNXZLXIg",
        "outputId": "29644266-7d2f-4b9a-cba4-d88c65864915"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "೫. ವ್ಯವಸ್ಥಾಪಕ ಉತ್ಪಾದಕತೆ ಸಂಬಂಧಪಟ್ಟಿದೆ.\n",
            "ಕಂದಹಾರ್ ವಿಮಾನ ನಿಲ್ದಾಣದ ಮೇಲೆ ರಾಕೆಟ್ ದಾಳಿ\n",
            "ಈಗಾಗಲೇ ಪಠ್ಯ ಬೋಧನೆ ಮುಗಿದಿದ್ದು ಪುನರಾವರ್ತನೆ ಆಗುತ್ತಿದೆ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBEw8Hi4Lb0-",
        "outputId": "df341408-a312-4476-f514-884c25d82776"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the result using bleu score"
      ],
      "metadata": {
        "id": "F8oMsErn4cOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu NLLB.hi-kn.kn-filtered.kn.subword.test.desubword -i NLLB.kn.translated.desubword -m bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_brQC6k08x5",
        "outputId": "405823bf-51f7-4381-e882-077686b78026"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 12.8,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"40.4/16.8/9.9/6.4 (BP = 0.887 ratio = 0.893 hyp_len = 14961 ref_len = 16763)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_test = 'NLLB.hi-kn.kn-filtered.kn.subword.test.desubword'\n",
        "target_pred = 'NLLB.kn.translated.desubword'\n",
        "\n",
        "refs = []\n",
        "preds = []\n",
        "\n",
        "def load_file(data_file, data_list):\n",
        "  with open(data_file) as test:\n",
        "    for line in test:\n",
        "      line = line.strip()\n",
        "      data_list.append(line)\n",
        "    return data_list\n",
        "\n",
        "\n",
        "refs = load_file(target_test, refs)\n",
        "preds = load_file(target_pred, preds)\n",
        "\n",
        "for i in range(2):\n",
        "  print(\"Reference sentence:\", refs[i])\n",
        "  print(\"Translated sentence:\", preds[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40IIpU593M42",
        "outputId": "3909e811-4bb8-44aa-b960-32daea6d3f2f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference sentence: ೫. ವ್ಯವಸ್ಥಾಪಕ ಉತ್ಪಾದಕತೆ ಸಂಬಂಧಪಟ್ಟಿದೆ.\n",
            "Translated sentence: ಉತ್ಪಾದಕತೆಗೆ ಸಂಬಂಧಿಸಿದ ಅಂಶಗಳು.\n",
            "Reference sentence: ಕಂದಹಾರ್ ವಿಮಾನ ನಿಲ್ದಾಣದ ಮೇಲೆ ರಾಕೆಟ್ ದಾಳಿ\n",
            "Translated sentence: ಕಂದಹಾರ್ ವಿಮಾನ ನಿಲ್ದಾಣದಲ್ಲಿ ರಾಕೆಟ್ ದಾಳಿ\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}